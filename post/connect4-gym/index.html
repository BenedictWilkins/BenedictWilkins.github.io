<!DOCTYPE HTML>

<html lang='en'>
	<head>
		<title>Connect 4 Gym &middot; Benedict Wilkins</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		
		<!-- plugins -->
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/bootstrap/bootstrap.min.css">
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/Ionicons/css/ionicons.min.css">
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/magnific-popup/magnific-popup.css">
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/slick/slick.css">
		

		<link rel="stylesheet" href='/assets/css/main.css' />
		
		
		<noscript><link rel="stylesheet" href='/assets/css/noscript.css' /></noscript>
		
	</head>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ["\\[","\\]"]],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
	<body lang='en' class="is-preload">

		
			<div id="wrapper">

                
<header id="header">
    <a href='/' class="logo">Benedict Wilkins</a>
</header>

                


<nav id="nav">
    <ul class="links">
        

    <li><a href=/>home</a></li>


        

    <li><a href=/mini-projects/>mini-projects</a></li>


        

    <li><a href=/about/>about</a></li>


        

        
            <li><a href='#footer'>Contact</a></li>
        
    </ul>
    
    <ul class="icons">
        
        <li><a href="https://twitter.com/BenWilkinsAI" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
        
        
        
        
        <li><a href="https://www.linkedin.com/in/benedictwilkins" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
        
        
        <li><a href="https://github.com/BenedictWilkinsAI/" class="icon fa-github"><span class="label">GitHub</span></a></li>
        
        
        <li><a href="https://www.youtube.com/channel/UCSYJwfY00hkrVOQBJJZK6fg" class="icon fa-youtube"><span class="label">Youtube</span></a></li>
        
        
        
        
    </ul>
    
</nav>


				
					<div id="main">

						
                        <section class="post">
                            <header class="major">
                                
                                <span class="date">November 17, 2020</span>
                                
                                <h1>Connect 4 Gym</h1>
                                <p>Building a connect 4 Robot: Episode 1</p>
                            </header>
                            
                            <p>My goal is to completely solve the game of Connect4 (also sometimes called <em>Four in a row</em>). By completely, I mean completely. Essentially, I am going to go through all the steps in making a fully functional connect 4 robot that can play a physical game, and yes that includes building a robotic arm, a vision system, a learning algorithm that will enable our agent to learn to play from scratch, and everything else in between that is needed to tie it all together. I will be documenting the work as a series of videos and blog posts, of which this is the first. Below outlines the multi-stage plan (not in order) for the project, just to give you an idea and help me to stay on track!</p>
<figure>
    <img src="/images/connect4/plan.png" width="95%"/> 
</figure>

<hr>
<h2 id="connect-4-basics">Connect 4 Basics</h2>
<p>If you are unfamilar with connect 4, the rules are simple. Players take turns to place counters in the top of a rack, the counters slide to the bottom under gravity into the nearest empty position. The first to make 4 in a row along a row,column or diagonal wins. If the rack is filled without a win, it is a draw. The typical size of a rack is 7 $\times$ 7, this is the size we will be working with initially. In the game below, the yellow player has won the game.</p>
<figure>
    <img src="/images/connect4/random.png" width="95%"/> 
</figure>

<hr>
<h2 id="connect-4--openai-gym">Connect 4 &amp; OpenAI Gym</h2>
<p>In this post the focus will be on implementing a simulator that can be used to train a self-play Reinforcement Learning agent. This will involve implementing a custom <a href="https://gym.openai.com/">OpenAI Gym</a> environment. If you are not familiar with Gym don&rsquo;t worry, it is great and I suggest taking a look at some of the other cool environments.</p>
<p>The basic outline of a Gym environment is as follows:</p>
<pre><code>import gym

class Connect4(gym.Env):

    def __init__(self):
        super(Connect4, self).__init__()

    def step(self, action):
        pass 

    def reset(self):
        pass 
</code></pre><p>The <code>step</code> function is called to evolve the environment given an agents action. In the case of connect 4 this corresponds to one of the two players dropping their counter in the top of the rack. As the game is turn based, the action will swap back an forth between placing a counter for player 1 and player 2.</p>
<p>There are three key aspects to a Gym environment - State, Action and Reward. We will go through each in order in the sections to come.</p>
<hr>
<h3 id="state">State</h3>
<p>The first step in creating the environment is defining what we mean by an environment <code>state</code>. Here a state is a particular configuration of counters. The state will be represented as a <code>numpy</code> array filled with 0s, 1s, and -1s, with 0 indicating an empty position, 1 indicating player 1&rsquo;s counter and -1 indicating players 2&rsquo;s counter.</p>
<pre><code>def __init__(self, n=7):
    super(Connect4, self).__init__()
    self.n = 7
    self.state = np.zeros((self.n, self.n), dtype=np.float32) # n x n grid
    self.index = np.zeros(self.n, dtype=np.int8) + self.n - 1 # record of placement positions
    self.turn = 1 # 1 or -1 depending on the turn

    self.observation_space = gym.spaces.Box(-1, 1,shape=self.state.shape, dtype=np.float32)
    self.done = False
</code></pre><p>The <code>state</code> is initially all 0s (no player has placed any counters yet). The <code>index</code> array keeps a record of the highest most counter in each column of the state, so that we know where to put the next counter in the column. For example:</p>
<pre><code>[[ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  1]
 [ 0  1  1  0  0 -1 -1]] 
 
 [ 6  5  5  6  6  5  4]
</code></pre><p>The <code>observation_space</code> defines what sort of state we are dealing with, it is generally used by people who want to make use of your environment so that they know what to expect. In this case, our <code>observation_space</code> is a <code>Box</code> space of size 7 $\times$ 7 where values are between -1 and 1. The reason I have chosen -1 and 1 will become clear in later posts (hint: we will be using neural networks).</p>
<p>Next, at each <code>step</code>, we should update the <code>state</code>.</p>
<pre><code>def step(self, action):
    self.state[self.index[action], action] = self.turn
    self.done, reward = False, 0

    self.index[action] -= 1
    self.turn = - self.turn
    
    return self.state, reward, self.done
</code></pre><p>The <code>state</code> is updated using our handy <code>index</code> array and the players action. The action is just an integer and so can also be used as an index (0-6) to indicate which column to drop a counter. In addition to updating the state, we update the <code>index</code> to take into account the counter that was just dropped, and swap the <code>turn</code>. We will come back to <code>reward</code> and <code>done</code> later. First, <code>reset</code> and then <code>action</code>.</p>
<p>The <code>reset</code> method is equally as important as <code>step</code>, it reverts the environment back to the initial state (a state of all 0s), so that the game may be played again. This is pretty straight forward to do and will look a lot like <code>__init__</code>.</p>
<hr>
<h3 id="action">Action</h3>
<p>In addition to the <code>observation_space</code> we need to define an <code>action_space</code>, in the <code>__init__</code> method.</p>
<pre><code>self.action_space = Connect4Discrete(self.n)
</code></pre><p>A player has the option of placing a counter in 1 of 7 columns, but what happens when a column is full? The action becomes illegal, we want this to be communicated some how by our action space. By default Gym doesn&rsquo;t provide this functionality, so we will create a new action space that is derived from Gym&rsquo;s <code>Discrete</code> action space.</p>
<pre><code>class Connect4Discrete(gym.spaces.Discrete):
    
    def __init__(self, n):
        super(Connect4Discrete, self).__init__(n)
    
    def sample(self, state):
        actions = np.arange(self.n)[self.mask(state)]
        assert len(actions) &gt; 0 # NO VALID ACTIONS IN THIS STATE, GAME OVER?
        return np.random.choice(actions)

    def mask(self, state): 
        return (1 - np.abs(state[0,:])).astype(np.bool) # 0 for invalid, 1 for valid

    def is_valid(self, state, action):
        return state[0, action] == 0
</code></pre><p>Our <code>action_space</code> defines a <code>mask</code> method, this method tells us which actions are valid for a particular state, see the example below.</p>
<pre><code>[[ 0  0  0  0  0  1  0]
 [ 0  0  0  0  0  1  0]
 [ 0 -1  0  0  0 -1 -1]
 [ 0  1  0  0  1  1 -1]
 [ 1  1 -1  0 -1 -1  1]
 [-1  1  1 -1  1 -1 -1]
 [-1 -1  1  1  1 -1  1]]

 [ 1  1  1  1  1  0  1]
</code></pre><p>In this state, a player can take any action with the exception of action 5 as the column is full. Actions are the same for both players, but the effect is different (either a 1 or -1 will be placed in the state).</p>
<hr>
<h3 id="reward">Reward</h3>
<p>Now comes a slightly tricker problem, how do we determine when the game is over? The drawing case is quite easy to deal with, we can just look at the top most row and if its filled then the game is over and its a draw.</p>
<pre><code>done = self.action_space.mask(self.state).sum()
</code></pre><p><code>done</code> indicates whether the game is over or not, in the above we take advantage of our newly defined <code>action_space</code> <code>mask</code>, essentially, if there are no valid actions, then all columns are full, the game is over and its a draw.</p>
<p>Checking whether there are 4 counters in a row is slightly more involved, especially if we want to do it in a <em>numpythonic</em> way (without loops). First some observations, when a counter is placed, the only places a new 4 in a row can appear is along the row, column or two diagonals that contain the new counter. This limits our search to only these regions of the state. Each region can be found using some clever indexing and the <code>diag</code> function. Notice also that we only need to look at the counters that are the same as the one for the current turn (i.e. only 1s if its player 1&rsquo;s turn, or only -1s if its player 2&rsquo;s turn).</p>
<pre><code>t = self.state[i,j] # current turn counter
h = self.state[i,:] == t
v = self.state[:,j] == t
lr = np.diag(self.state, k = j - i) == t
rl = np.diag(np.fliplr(self.state), k = (self.n - 1 - j) - i)
</code></pre><p>The example below illustrates these operations, where player 2 has wont the game in the horizontal.</p>
<pre><code>action = 3  # place in column 3
turn = -1   # player 2's turn
state = 
    [[ 0  0  0  0  0  1  0]
     [ 0  0  0  0  0  1  0]
     [ 0 -1  0  0  0 -1 -1]
     [ 0  1  0  0  1  1 -1]
     [ 1  1 -1 -1 -1 -1  1]
     [-1  1  1 -1  1 -1 -1]
     [-1 -1  1  1  1 -1  1]]

h  = [ 0  0  1  1  1  1  0] 
v  = [ 0  0  0  0  1  1  0]
lr = [ 0  1  0  1  0  1]
rl = [ 0  1  0  1  0  1]
</code></pre><p>Now the question is, how do we extract the string of 4 1&rsquo;s to indicate a win? the following function does just this:</p>
<pre><code>def _done(x):
    idx, = np.diff(x).nonzero()
    idx += 1
    if x[0]:
        idx = np.r_[0, idx]
    if x[-1]:
        idx = np.r_[idx, x.size]
    idx = idx.reshape(-1, 2)
    return np.any(np.diff(idx, axis=1) &gt;= 4)
</code></pre><p>Lets try to understand what is happening here, <code>diff</code> looks at the consecutive difference of each element in <code>x</code> (where <code>x</code> is one the arrays described above) i.e. <code>x[i+1] - x[i]</code>.  Working with the row array (<code>h</code>) above, this will produce:</p>
<pre><code>np.diff([0, 0, 1, 1, 1, 1, 0])
&gt;&gt;&gt; [0, 1, 0, 0, 0, -1]
</code></pre><p><code>.nonzero()</code> gives us the indices of the non-zero entries of an array, in this case, <code>[1, 5]</code>. The next part of the function fixes the ends of the <code>idx</code> array to deal with the edge cases (<code>diff</code> looses some information about the two edge elements). We then <code>reshape</code> to get pairs of indices, and given that the size of the <code>idx</code> is always even (check for yourself), this gives us pairs that we can be sure indicate regions in the original array that contain only 1s (or -1s). We can then simply subtract elements in the pairs of indices to get the size of the region and check if its <code>&gt;= 4</code>. If we apply <code>_done</code> to the row, column and diagonals, it will tell us whether a given player has won.</p>
<p>So what about reward? The reward is used by a Reinforcement Learning (RL) agent as a guide for its behaviour, an RL agent tries to maximise its reward, so high rewards indicate desirable outcomes. If we want to train an agent to win, we should reward it with a positive reward when it wins, and possibly punish it with a negative reward when it looses. We prefer than an agent draws over loosing as in some situations it may only be possible for an agent to draw, we therefore assign a higher reward for drawing than for loosing, and still higher for winning.
For now we might assign 0 to each player for a draw, +1 to the winner and -1 to the looser. Anyway, enough on RL this will be discussed in much more detail in later posts.</p>
<p>Putting everything together gives us a complete connect 4 game as a Gym environment that we can use later to train a RL agent!</p>
<p>The full code can be found in the <a href="https://github.com/BenedictWilkinsAI/connect4/blob/main/connect4/envs/__init__.py">git repository</a> if you want to run it yourself. The repo will be the central hub for the project along with my <a href="https://www.youtube.com/channel/UCSYJwfY00hkrVOQBJJZK6fg?view_as=subscriber">youtube channel</a> where I hope to post some more in-depth explanations and demos. I hope this post has given you some insights into OpenAI gym, and perhaps laid the foundation for future posts. Thanks for reading!</p>


                            
                            
                            
                            
                            
                        </section>

					</div>

                    

<footer id="footer">
    
    <section class="split contact">
        
        
        
        <section>
            <h3>Email</h3>
            <p><a href="mailto:benrjw@googlemail.com">benrjw@googlemail.com</a></p>
        </section>
        
        
        <section>
            <h3>Social</h3>
            <ul class="icons alt">
                
                <li><a href="https://twitter.com/BenWilkinsAI" class="icon alt fa-twitter"><span class="label">Twitter</span></a></li>
                
                
                
                
                <li><a href="https://www.linkedin.com/in/benedictwilkins" class="icon alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
                
                
                <li><a href="https://github.com/BenedictWilkinsAI/" class="icon alt fa-github"><span class="label">GitHub</span></a></li>
                
                
                <li><a href="https://www.youtube.com/channel/UCSYJwfY00hkrVOQBJJZK6fg" class="icon alt fa-youtube"><span class="label">Youtube</span></a></li>
                
                
                
                
            </ul>
        </section>
        
    </section>
</footer>

                    
<div id="copyright">
    <ul><li>&copy; Benedict Wilkins</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li><li>Hugo Port: <a href="https://curtistimson.co.uk">curttimson</a></li></ul>
</div>


            </div>
            
            
<script src='/assets/js/jquery.min.js'></script>
<script src='/assets/js/jquery.scrollex.min.js'></script>
<script src='/assets/js/jquery.scrolly.min.js'></script>
<script src='/assets/js/browser.min.js'></script>
<script src='/assets/js/breakpoints.min.js'></script>
<script src='/assets/js/util.js'></script>
<script src='/assets/js/main.js'></script>

	</body>
</html>
