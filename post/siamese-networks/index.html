<!DOCTYPE HTML>

<html lang='en'>
	<head>
		<title>Siamese Networks &middot; Benedict Wilkins</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		
		<!-- plugins -->
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/bootstrap/bootstrap.min.css">
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/Ionicons/css/ionicons.min.css">
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/magnific-popup/magnific-popup.css">
		
		<link rel="stylesheet" href="https://benedictwilkinsai.github.io/plugins/slick/slick.css">
		

		<link rel="stylesheet" href='/assets/css/main.css' />
		
		
		<noscript><link rel="stylesheet" href='/assets/css/noscript.css' /></noscript>
		
	</head>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
	<body lang='en' class="is-preload">

		
			<div id="wrapper">

                
<header id="header">
    <a href='/' class="logo">Benedict Wilkins</a>
</header>

                


<nav id="nav">
    <ul class="links">
        

    <li><a href=/>home</a></li>


        

    <li><a href=/mini-projects/>mini-projects</a></li>


        

    <li><a href=/about/>about</a></li>


        

        
            <li><a href='#footer'>Contact</a></li>
        
    </ul>
    
    <ul class="icons">
        
        <li><a href="https://twitter.com/BenWilkinsAI" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
        
        
        
        
        <li><a href="https://www.linkedin.com/in/benedictwilkins" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
        
        
        <li><a href="https://github.com/BenedictWilkinsAI/" class="icon fa-github"><span class="label">GitHub</span></a></li>
        
        
        <li><a href="https://www.youtube.com/channel/UCSYJwfY00hkrVOQBJJZK6fg" class="icon fa-youtube"><span class="label">Youtube</span></a></li>
        
        
        
        
    </ul>
    
</nav>


				
					<div id="main">

						
                        <section class="post">
                            <header class="major">
                                
                                <span class="date">September 26, 2020</span>
                                
                                <h1>Siamese Networks</h1>
                                <p>Implementing Siamese Networks and Triplet Loss in Pytorch</p>
                            </header>
                            
                            <p>Recently I have been looking into Siamese networks in my research, for my own understanding I like to implement new models that I come across - if you can code it you must understand it! Lets begin.</p>
<h2 id="what-is-a-siamese-network">What is a Siamese Network?</h2>
<p>There are some great resources on the web that answer this question, in particular this <a href="https://www.youtube.com/watch?v=6jfw8MuKwpI">short course</a> by distinguished researcher Andrew Ng. For completeness I will explain the idea briefly here.</p>
<p>Siamese networks learn a distance measure between pairs of labelled examples. They work as follows, examples are fed into the network and the network creates an encoding (simply the output of the network) for each example. As usual the encodings are used in an objective function. The distinguishing feature of Siamese networks as compared with the usual neural network set up is that the objective function uses multiple examples for a single loss computation. In this post we will focus on the <strong>triplet loss</strong> function. The objective is to learn a distance measure that keeps examples with the same label close together in the encoding space, while at the same time keeping examples with different labels far from each other. To accomplish this triplet loss uses three examples &ndash; anchor, positive and negative. The anchor and positive examples share the same label, anchor and negative have different labels. Triplet loss is defined as follows:</p>
<p>$$\mathcal{L}(A,P,N) = max(|| f(A) - f(P) || - || f(A) - f(N) || + \alpha, 0)$$</p>
<p>where $A,P,N$ are the anchor, positive and negative examples respectively, $\alpha$ is a constant value that prevents the network from learning trivial solutions and $||\cdot||$ is a norm (e.g. $L_2$). Triplet loss is explained in much more detail in Andrew Ng&rsquo;s <a href="https://www.youtube.com/watch?v=d2XB5-tuCWU">course</a>.</p>
<hr>
<h2 id="data">Data</h2>
<p>For our experiments we are going to use the famous MNIST dataset, first <a href="https://www.kaggle.com/benedictwilkinsai/mnist-hd5f">download</a> the dataset. It can be loaded with the following code with the <a href="https://www.h5py.org/">h5py</a> python module, the module can be installed with: <code>pip install h5py</code>. After extracting the archive into your working directory, the data can be loaded as follows:</p>
<pre><code>import h5py

def mnist():
    #load train
    f = h5py.File(&quot;./train.hdf5&quot;, 'r')
    train_x, train_y = f['image'][...], f['label'][...]
    f.close()

    #load test
    f = h5py.File(&quot;./test.hdf5&quot;, 'r')
    test_x, test_y = f['image'][...], f['label'][...]
    f.close()
    
    return train_x[:,np.newaxis], train_y, test_x[:,np.newaxis], test_y
    
</code></pre><p>We add a new axis to <code>train_x</code> and <code>test_x</code> to ensure the data follows the PyTorch image format convention $(N,C,H,W)$.</p>
<hr>
<h2 id="code">Code</h2>
<p>I am going to race through creating our neural network in Pytorch. There are many explanations available online for the following code, and I will be releasing a post soon that goes through it in detail.</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class CNet(nn.Module):

    def __init__(self, input_shape):
        super(CNet, self).__init__() 
        self.input_shape = input_shape
        
        self.conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=4, stride=2)
        self.conv2 = nn.Conv2d(64, 32, kernel_size=4, stride=1)
        self.conv3 =  nn.Conv2d(32, 16, kernel_size=4, stride=1)
        
        s1 = conv_output_shape(input_shape[1:], kernel_size=4, stride=2)
        s2 = conv_output_shape(s1, kernel_size=4, stride=1)
        s3 = conv_output_shape(s2, kernel_size=4, stride=1)
        
        self.output_shape = np.prod(s3) * 16
    
    def to(self, device):
        self.device = device
        return super(CNet, self).to(device)

    def forward(self, x_):
        x_ = x_.to(self.device)
        y_ = F.leaky_relu(self.conv1(x_))
        y_ = F.leaky_relu(self.conv2(y_))
        y_ = F.leaky_relu(self.conv3(y_)).view(x_.shape[0], -1)
        return y_
    
class CNet2(CNet):
    
    def __init__(self, input_shape, output_shape, activation=lambda x: x):
        super(CNet2, self).__init__(input_shape)
        self.out_layer = nn.Linear(self.output_shape, output_shape)
        self.output_shape = output_shape
        self.activation = activation
        
    def forward(self, x_):
        x_ = super(CNet2, self).forward(x_)
        y_ = self.activation(self.out_layer(x_))
        return y_

def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):
    from math import floor
    if type(kernel_size) is not tuple:
        kernel_size = (kernel_size, kernel_size)
    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)
    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)
    return h, w
</code></pre><p>We have defined a simple network with 3 convolutional layers and a single fully connected output layer in <code>CNet2</code>. The next step is to start thinking about how to implement the triplet loss.</p>
<p>First we need to implement $||\cdot||$, the distance between different data points. As we will be using stochastic gradient descent to optimise our model, we will be computing distances over batches. For each batch we compute a distance matrix for both positive and negative examples. The following code computes the squared $L_2$ distance matrix.</p>
<pre><code>def distance_matrix(x1, x2=None):
    if x2 is None:
        x2 = x1
    n_dif = x1.unsqueeze(1) - x2.unsqueeze(0)
    return torch.sum(n_dif * n_dif, -1)
</code></pre><p>This function is pretty memory in-efficient, but for our purposes its OK.</p>
<p>Now for the loss computation, we will be using the labels that MNIST has (0-9) to determine $N$ and $P$. For each label $i$, our positive images $P$ are those labelled $i$ and our negative are any image not labelled $i$.</p>
<pre><code>def loss(model, x, y, margin=0.2):
    x_ = model(x)
    unique = np.unique(y)
    device = list(model.parameters())[0].device
    loss = torch.FloatTensor(np.array([0.])).to(device)

    for u in unique:
        pi = np.nonzero(y == u)[0]
        ni = np.nonzero(y != u)[0]
        
        #slightly more efficient below
        xp_ = x_[pi] # get all positive images
        xn_ = x_[ni] # get all negative images
        xp = distance_matrix(xp_, xp_) #P-P distance
        xn = distance_matrix(xp_, xn_) #P-N distance

        #3D tensor, (a - p) - (a - n) 
        xf = xp.unsqueeze(2) - xn

        xf = F.relu(xf + margin) #triplet loss
        loss += xf.sum()

    return loss
</code></pre><p>In the above, we loop through each of the digits, compute the relevant distances, then the triplet objective and finally aggregate (sum) the losses for each digit. We use the each positive example as an anchor example, after finishing the loop all of the examples will have been used as an anchor.</p>
<p>Now we define our model, optimiser and some hyper parameters.</p>
<pre><code>input_dim = (1, 28, 28)
batch_size = 100
margin = 0.2
latent_dim = 2
lr = 0.0005
epochs = 4

if torch.cuda.is_available(): 
    device = 'cuda'
else:
    device = 'cpu'
print(&quot;USING DEVICE:&quot;, device)

x_train, y_train, x_test, y_test = mnist()
x_train = torch.FloatTensor(x_train).to(device)
x_test = torch.FloatTensor(x_test).to(device)
model = CNet2(input_dim, latent_dim).to(device)

optim = torch.optim.Adam(model.parameters(), lr=lr)
</code></pre><p>If you have cuda enabled GPU things will be alot faster. Before starting the training, we want to be able to visualise the results. If we use a latent dimension of 2, we can visualise the output of the network easily in a plot. Lets implement that now.</p>
<pre><code>import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [8,4.5] #set the figure size

def plot(fig, model, x, y):
    plt.clf()
    with torch.no_grad():
        z = model(x).cpu().numpy()
    for i in range(0,10):
        plt.scatter(*z[y==i].T, marker=&quot;.&quot;, alpha=0.5, edgecolors='none')
    plt.legend([str(i) for i in range(0,10)], loc=&quot;upper right&quot;)
    fig.canvas.draw()
</code></pre><p>The above will plot the output of our network in 2D and assign different colours to each digit. The extra bits (<code>draw</code>, <code>clf</code>) ensure that the plot is efficiently updated in real time in a Jupyter notebook. Give it a quick test with <code>plot(fig, model, x_test, y_test)</code> to get a plot similar to below.</p>
<figure>
    <img src="/images/siamese/initial.png" width="95%"/> 
</figure>

<p>Finally on to the training, we optimise the model in the usual way below:</p>
<pre><code>x_train = x_train.reshape(x_train.shape[0] // batch_size, batch_size, *x_train.shape[1:])
y_train = y_train.reshape(y_train.shape[0] // batch_size, batch_size, *y_train.shape[1:])

for e in range(epochs):
    for x,y in zip(*[x_train, y_train]):
        optim.zero_grad()
        _loss = loss(model, x, y, margin=margin)
        _loss.backward()
        optim.step()
        
        plot(fig, model, x_test, y_test)
</code></pre><p>A little trick for an efficient iterator over batches is to reshape the dataset as above (the only downside being that the dataset and batch size have to be compatible.) Our training set of size <code>(60000, 1, 28, 28)</code> becomes a batched training set of size <code>(6000, 100, 1, 28, 28)</code> with a batch size of 100. We are also plotting the test set at each iteration to get a smooth visualisation of the training, this is quite expensive so it might be worth only plotting every $n$ steps on slower computers.</p>
<p>We end up with a visualisation like below when we run all of our code.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/WcDAPGmZlXc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>

<p>The full code is available as a <a href="https://gist.github.com/BenedictWilkinsAI/d58bcecc48eaf0553320484ee7eda040">notebook</a>. Thanks for reading!</p>


                            
                            
                            
                        </section>

					</div>

                    

<footer id="footer">
    
    <section class="split contact">
        
        
        
        <section>
            <h3>Email</h3>
            <p><a href="mailto:benrjw@googlemail.com">benrjw@googlemail.com</a></p>
        </section>
        
        
        <section>
            <h3>Social</h3>
            <ul class="icons alt">
                
                <li><a href="https://twitter.com/BenWilkinsAI" class="icon alt fa-twitter"><span class="label">Twitter</span></a></li>
                
                
                
                
                <li><a href="https://www.linkedin.com/in/benedictwilkins" class="icon alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
                
                
                <li><a href="https://github.com/BenedictWilkinsAI/" class="icon alt fa-github"><span class="label">GitHub</span></a></li>
                
                
                <li><a href="https://www.youtube.com/channel/UCSYJwfY00hkrVOQBJJZK6fg" class="icon alt fa-youtube"><span class="label">Youtube</span></a></li>
                
                
                
                
            </ul>
        </section>
        
    </section>
</footer>

                    
<div id="copyright">
    <ul><li>&copy; Benedict Wilkins</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li><li>Hugo Port: <a href="https://curtistimson.co.uk">curttimson</a></li></ul>
</div>


            </div>
            
            
<script src='/assets/js/jquery.min.js'></script>
<script src='/assets/js/jquery.scrollex.min.js'></script>
<script src='/assets/js/jquery.scrolly.min.js'></script>
<script src='/assets/js/browser.min.js'></script>
<script src='/assets/js/breakpoints.min.js'></script>
<script src='/assets/js/util.js'></script>
<script src='/assets/js/main.js'></script>

	</body>
</html>
